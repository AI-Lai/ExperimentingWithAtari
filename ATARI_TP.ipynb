{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "ATARI-TP.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AI-Lai/ExperimentingWithAtari/blob/main/ATARI_TP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiBz2qsRSk8W"
      },
      "source": [
        "# 0. Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICCzVw6kSk8Z"
      },
      "source": [
        "%pip install tensorflow\n",
        "!pip install pyglet\n",
        "print(\"done\")\n",
        "!pip install gym keras-rl2 gym[atari]\n",
        "print(\"done\")\n",
        "%pip install -U gym>=0.21.0\n",
        "print(\"done\")\n",
        "%pip install -U gym[atari,accept-rom-license]\n",
        "print(\"DONE\")\n",
        "#!pip install pyvirtualdisplay\n",
        "#!pip install reverb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRReaMFhSk8b"
      },
      "source": [
        "# 1. Test Environment in OpenAI Gym"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpmFQ4NNMFgy"
      },
      "source": [
        "Import gym and random: this will enable us to see what's happening inside the game.\n",
        "\n",
        "Ideally the lines \n",
        "\n",
        "\"!pip install gym keras-rl2 gym[atari]\n",
        "\n",
        "%pip install -U gym>=0.21.0\n",
        "\n",
        "%pip install -U gym[atari,accept-rom-license]\" \n",
        "\n",
        "allow us to run any atari gym environment without preinstalling anything.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ra1MWTfSk8c",
        "outputId": "8e721a42-2d8d-4c79-d5c2-40c0d059dfcc"
      },
      "source": [
        "#todo\n",
        "game= \"gamename\"\n",
        "env = gym.make(game)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ale_py/roms/utils.py:90: DeprecationWarning: SelectableGroups dict interface is deprecated. Use select.\n",
            "  for external in metadata.entry_points().get(self.group, []):\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cg5DJZojM09F"
      },
      "source": [
        "We can try to run some episodes to see what's happening in the game.\n",
        "For the moment we can just run random actions.\n",
        "\n",
        "In order to visualize the game, you can call **env.render()**, possibly in mode \"human\". At the end of each event print the results.\n",
        "\n",
        "Note: render does not work on colab, try it in your own notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPlC6AYJSk8d",
        "outputId": "957e7140-8fb0-4d9d-fc6d-b513fdf28725"
      },
      "source": [
        "episodes = 3\n",
        "for episode in range(episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    score = 0 \n",
        "    \n",
        "    while not done:\n",
        "      pass\n",
        "      \n",
        "    print('Episode:{} Score:{}'.format(episode+1, score))\n",
        "env.close()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode:1 Score:50.0\n",
            "Episode:2 Score:105.0\n",
            "Episode:3 Score:55.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5ImTfXOSk8e"
      },
      "source": [
        "# 2. Create a Deep Learning Model with Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wxayj_JlN0xJ"
      },
      "source": [
        "Import the different libraries (numpy, keras.models.Sequential(), the different layers, and Adam for the optimisation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0isQ_LeSk8e"
      },
      "source": [
        "import numpy as np\n",
        "# import sequential, Dense, Flatten, Convolution2D, adam\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pgNFVkmOIBG"
      },
      "source": [
        "Build a CNN. The structure can be adapted, but we can use as a starting point the structure used in https://www.nature.com/articles/nature14236/ and https://arxiv.org/abs/1511.06581\n",
        "\n",
        "Since there are some sparse bugs, wrap the model in a function build_model just in case of necessity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inK57A85Sk8f"
      },
      "source": [
        "def build_model(height, width, channels, actions):\n",
        "    model = Sequential()\n",
        "    model.add(Convolution2D(32, (8,8), strides=(4,4), activation='relu', input_shape=(3,height, width, channels)))\n",
        "    ###### layers\n",
        "    model.add(Dense(actions, activation='linear'))\n",
        "    return model\n",
        "# del model\n",
        "# in case of random error, rebuild the model\n",
        "model = build_model(height, width, channels, actions)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6uYNr95PNC5"
      },
      "source": [
        "As usual, before startin to mess up everything, we can peek a look at our network.\n",
        "\n",
        "If it seems too massive for your architechture you can prune it, just for the sake of seeing it working"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyJuU1RcSk8h"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QftbbFigSk8i"
      },
      "source": [
        "# 3. Build Agent with Keras-RL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9D3pp5ePiWk"
      },
      "source": [
        "Ok, now the interesting part.\n",
        "\n",
        "Using classical Q-Learning/SARSA is not feasible, can you tell why?\n",
        "\n",
        "Indeed, we will use the CNN built in the previous paragraph to \"suggest\" to the agent the best action at each step.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mcn4uH2XQSnC"
      },
      "source": [
        "Here we import from keras the libraries to build and train such an agent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RR4d5KVcSk8i"
      },
      "source": [
        "from rl.agents import DQNAgent\n",
        "from rl.memory import SequentialMemory\n",
        "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkRFcAceQfrO"
      },
      "source": [
        "We can build our agent.\n",
        "We specify a policy and a memory. We can also set up a dueling network as explained in  https://arxiv.org/abs/1511.06581"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ty4B0Qy-Sk8j"
      },
      "source": [
        "def build_agent(model, actions):\n",
        "    policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.2, nb_steps=10000)\n",
        "    memory = SequentialMemory(limit=1000, window_length=3)\n",
        "    dqn = DQNAgent(model=model, memory=memory, policy=policy,\n",
        "                  enable_dueling_network=True, dueling_type='avg', \n",
        "                   nb_actions=actions, nb_steps_warmup=1000\n",
        "                  )\n",
        "    return dqn\n",
        "dqn = build_agent(model, actions)\n",
        "dqn.compile(Adam(lr=1e-4))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8LevRKbSs-4"
      },
      "source": [
        "We can now fit our dqn on our environment.\n",
        "We should specify the number of steps: for this kind of problem we need among 1M and 10M steps in order to reach pseudo-human (or even super-human) level.\n",
        "\n",
        "Nevertheless, it can be a good exercice to try out on 10K steps. If everything worked fine it will take more or less half an hour, the time for you to take a coffee and enjoy the different papers proposed in the TP.\n",
        "\n",
        "\n",
        "\n",
        "Lastly, it is very funny to set the parameter visualize to True, in order to watch what is effectively learning. Note that this operation will make your fit function way slower (unbearably slower)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pb-C-uevSk8k"
      },
      "source": [
        "#???? fit(env, steps, )"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nOO-8CfXkAo"
      },
      "source": [
        "Ideally, we can now test the performance of our agent.\n",
        "Again, we can print to screen the AI playing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTs1q1adSk8k"
      },
      "source": [
        "scores = dqn.test(env, nb_episodes=5, visualize=True)\n",
        "print(np.mean(scores.history['episode_reward']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-BnPtNUSk8l"
      },
      "source": [
        "# 4. Reloading Agent from Memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBhIQBVpUau3"
      },
      "source": [
        "If you got here you realised a very bad Atari player, good job.\n",
        "\n",
        "The objective can be to download the weights calculated by some other nice ppl on the web.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liQlvYkwSk8l"
      },
      "source": [
        "dqn.save_weights('bad_dqn_weights.h5f')"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wdwyrWxSk8m"
      },
      "source": [
        "del model, dqn\n",
        "#rebuild model and dqn after this"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkdGXf6HSk8m"
      },
      "source": [
        "dqn.load_weights('good_dqn_weights.h5f')"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2SfdvJ1XrcN"
      },
      "source": [
        "Up to you.\n",
        "What could we do to improve performance?\n",
        "Here are some suggestions and explications to get better results.\n",
        "https://towardsdatascience.com/tutorial-double-deep-q-learning-with-dueling-network-architectures-4c1b3fb7f756\n",
        "\n",
        "\n",
        "https://medium.com/analytics-vidhya/building-a-powerful-dqn-in-tensorflow-2-0-explanation-tutorial-d48ea8f3177a\n",
        "\n",
        "https://github.com/EvolvedSquid/tutorials/tree/master/dqn\n",
        "\n",
        "https://www.freecodecamp.org/news/an-introduction-to-deep-q-learning-lets-play-doom-54d02d8017d8/\n",
        "\n",
        "(the last one is quite outdated, now, but still presents some interesting points)\n",
        "\n",
        "Try to apply the suggestions !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FyYlUKPSk8n"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}